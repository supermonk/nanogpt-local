{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "44dfdbeb-8b5b-49d1-be5a-701e42874205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
    "import torch # we use PyTorch: https://pytorch.org\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "3a49f40e-3028-41ef-80db-419a2816cddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-11-06 02:51:50--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8002::154, 2606:50c0:8003::154, 2606:50c0:8000::154, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8002::154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt.19’\n",
      "\n",
      "input.txt.19        100%[===================>]   1.06M  --.-KB/s    in 0.1s    \n",
      "\n",
      "2025-11-06 02:51:51 (9.66 MB/s) - ‘input.txt.19’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Hyper parameters\n",
    "batch_size = 32 # how many independent sequences will we process in parallel? , Parallelization rate.\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_emd = 32\n",
    "# --------------\n",
    "\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "\n",
    "# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "# read it in to inspect it\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Let's now split up the data into train and validation sets\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train','val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X,Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c65b016e-40ed-4267-a6cd-ad1d0b07ac1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 6,  1, 57, 61, 53, 56, 52,  6],\n",
      "        [61,  1, 58, 53,  5, 58, 12,  0],\n",
      "        [58, 46, 39, 58,  1, 47, 57,  1],\n",
      "        [32, 47, 58, 59, 57,  1, 24, 39],\n",
      "        [52, 53, 40, 50, 43, 57,  8,  0],\n",
      "        [51, 43, 12,  0,  0, 22, 33, 24],\n",
      "        [47, 58, 47, 53, 52,  6,  1, 44],\n",
      "        [46, 43, 56, 43,  1, 52, 53, 61],\n",
      "        [42,  1, 57, 39, 63, 57,  1, 46],\n",
      "        [ 1, 57, 43, 43,  1, 58, 46, 43],\n",
      "        [42,  0, 32, 53,  1, 50, 47, 44],\n",
      "        [58, 63,  8,  0,  0, 22, 33, 24],\n",
      "        [58, 43, 41, 58, 53, 56, 12,  1],\n",
      "        [ 1, 58, 46, 43,  1, 50, 39, 61],\n",
      "        [46, 39, 56, 42,  6,  1, 24, 53],\n",
      "        [10,  0, 21,  1, 61, 47, 50, 50],\n",
      "        [58, 46, 43,  1, 39, 52, 58, 47],\n",
      "        [33, 25, 21, 27, 10,  0, 13,  1],\n",
      "        [43, 50, 47, 49, 43,  6,  1, 39],\n",
      "        [55, 59, 43, 43, 52,  6,  1, 63],\n",
      "        [43, 52, 58,  8,  0,  0, 18, 30],\n",
      "        [56, 12,  0,  0, 31, 13, 25, 28],\n",
      "        [53, 56, 58,  1, 46, 47, 57,  1],\n",
      "        [46, 39, 58,  1, 60, 53, 47, 41],\n",
      "        [ 1, 57, 46, 39, 50, 50,  1, 41],\n",
      "        [63, 53, 59, 56,  1, 50, 53, 56],\n",
      "        [57,  1, 17, 42, 61, 39, 56, 42],\n",
      "        [41, 43, 58, 46,  1, 52, 53, 58],\n",
      "        [42,  1, 47, 44,  1, 63, 53, 59],\n",
      "        [61, 47, 58, 46,  1, 46, 47, 57],\n",
      "        [ 1, 63, 53, 59, 56,  1, 45, 56],\n",
      "        [57,  1, 61, 47, 44, 43,  1, 47]])\n"
     ]
    }
   ],
   "source": [
    "print(xb) # our input to the transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa327258-baf9-4943-8ab2-9fdf37fed9a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "7449f451-6c24-4667-938a-9a058ff0447f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- HEAD MODULE ---\n",
    "class Head(nn.Module):\n",
    "    \"\"\" One head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
    "        # register lower triangular mask for causal attention\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        k = self.key(x)    # (B, T, head_size)\n",
    "        q = self.query(x)  # (B, T, head_size)\n",
    "\n",
    "        # compute attention weights\n",
    "        wei = q @ k.transpose(-2, -1) * (k.shape[-1] ** -0.5)  # (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "        # weighted aggregation of values\n",
    "        v = self.value(x)\n",
    "        out = wei @ v  # (B, T, head_size)\n",
    "        return out\n",
    "\n",
    "\n",
    "# --- MULTI-HEAD ATTENTION ---\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # concatenate output from each head\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        return out\n",
    "\n",
    "\n",
    "# --- BIGRAM LANGUAGE MODEL ---\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "        self.sa_heads = MultiHeadAttention(4, n_embed // 4)  # 4 heads\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        tok_emb = self.token_embedding_table(idx)  # (B, T, n_embed)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T, n_embed)\n",
    "        x = tok_emb + pos_emb  # (B, T, n_embed)\n",
    "\n",
    "        x = self.sa_heads(x)\n",
    "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # flatten batch and time for loss\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]  # crop context\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :]  # last time step\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ff1496fb-579b-4d05-ac5e-b0b1a7d00004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.007553  M parameters\n",
      "stepp 0: trian loss 4.1452, val loss 4.1477\n",
      "stepp 500: trian loss 2.6505, val loss 2.6823\n",
      "stepp 1000: trian loss 2.4924, val loss 2.5142\n",
      "stepp 1500: trian loss 2.4330, val loss 2.4183\n",
      "stepp 2000: trian loss 2.3832, val loss 2.3853\n",
      "stepp 2500: trian loss 2.3376, val loss 2.3436\n",
      "stepp 3000: trian loss 2.3094, val loss 2.3323\n",
      "stepp 3500: trian loss 2.2821, val loss 2.3085\n",
      "stepp 4000: trian loss 2.2791, val loss 2.3018\n",
      "stepp 4500: trian loss 2.2468, val loss 2.3002\n",
      "stepp 4999: trian loss 2.2536, val loss 2.2841\n",
      "\n",
      "A:\n",
      "Whame bres shing their ing lay tiths teray, all tawiu of ownearsces?\n",
      "\n",
      "Thishe youge: thrint on loome you himammene surd I cour nons ho oforme ay to feor the onit alll ris be\n",
      "you me dis my, komy some dearer Grwand,\n",
      "An theasres of kprowngce thou, of hesh is's, hun on faul yrow dantc; tomes but thy hin nid thes:\n",
      "Beshership bre angch erar wor\n",
      "aughd, now'd Dou wis sith wooonk!\n",
      "Wa hersour I haveentr! Cothe to fron haven noud lard:\n",
      "ArPENCIO:\n",
      "Cout ont jithe havet cour hacooull shonlou ow be ove'ld\n",
      "RIM\n"
     ]
    }
   ],
   "source": [
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, ' M parameters')\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    if iter % eval_interval == 0 or iter == max_iters -1:\n",
    "      losses = estimate_loss()\n",
    "      print(f\"stepp {iter}: trian loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    xb,yb = get_batch('train')\n",
    "\n",
    "    logits, loss = m(xb,yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6ec2a3-5c4e-4647-ae2c-44aba7585aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only code-example left\n",
    "# version 4 : self attention\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value  = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x) # B, T, 16\n",
    "q = query(x) # B, T, 16\n",
    "wei = q @k.transpose(-2,-1) # B, T, 16 @ # B, 16, T ---> B, T, T\n",
    "\n",
    "tril = torch.tril((torch.ones(T,T)))\n",
    "#wei = torch.zeros((T,T))\n",
    "# important for encoder, for decoder we dont want to see the future\n",
    "# wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "\n",
    "v = value(x)\n",
    "out = wei @ x\n",
    "out.shape\n",
    "\n",
    "v"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Jup-Stanford",
   "language": "python",
   "name": "jup-stanford"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
